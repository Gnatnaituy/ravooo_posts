+++
title="语言模型 & 马尔科夫假设"
tags=["math"]
categories=["Math"]
date="2022-01-04T17:32:08+08:00"
+++

### 链式法则

链式法则是概率论中一个常用法则。它使用一系列条件概念率和边缘概率，来推导联合概率，我用一个公式来给你看看它的具体表现形式。

$$
P(x_1,x_2,...,x_n) = P(x_1)*P(x_2|x_1)*P(x_3|x_1,x_2)*...*P(x_n|x_1,x_2,...,x_{n-1})
$$

其中，$x_1$ 到 $x_n$ 表示了 $n$ 个随机变量。

### 马尔科夫假设

任何一个词 $w_i$ 出现的概率只和它前面的 1 个或若干个词有关。

基于这个假设，我们可以提出**多元文法**（Ngram）模型。Ngram 中的“$N$”很重要，它表示任何一个词出现的概率，只和它前面的 $N-1$ 个词有关。

- $P(w_3|w_1,w_2)$ 出现概率很低，$P(w_4|w_1,w_2,w_3)$ 出现的概率就更低了。一直到 $P(w_n|w_1, w_2, ..., w_{n-1})$，基本上又为 0 了。
- 除此之外， $P(w_1, w_2, ..., w_n)$ 和 $P(w_n|w_1, w_2, ..., w_{n-1})$ 还不只会导致 0 概率，它还会使得模型存储空间的急速增加。

如何解决 0 概率和高复杂度的问题呢？马尔科夫假设和多元文法模型能帮上大忙了。如果我们使用三元文法模型，上述公式可以改写为：

$$
P(w_1,w_2,...,w_n)\approx P(w_1)*P(w_2|w_1)*P(w_3|w_2,w_1)*P(w_4|w_3,w_2)*...*P(w_n|w_{n-1},w_{n-2})
$$

### 语言模型的应用

**信息检索**

一种常见的做法是计算 P(d∣q)，其中 q 表示一个查询，d 表示一篇文档。P(d∣q) 表示用户输入查询 q 的情况下，文档 d 出现的概率是多少？如果这个概率越高，我们就认为 q 和 d 之间的相关性越高。

通过贝叶斯定理，可以将 P(d∣q) 重写如下：

$$
P(d|q) = \frac{P(q|d)*P(d)}{P(q)}
$$

对于同一个查询，其出现概率 P(q) 都是相同的，同一个文档 d 的出现概率 P(d) 也是固定的。因此它们可以忽略，我们只要关注如何计算 P(q∣d)。而语言模型，为我们解决了如何计算 P(q∣d) 的问题，让 k1,k2,…,kn 表示查询 q 里包含的 n 个关键词。那么根据之前的链式法则公式，可以重写为这样：

$$
P(q|d) = P(k_1,k_2,k_3,...,k_n|d) = P(k_1|d) * P(k_2|k_1,d) * P(k_3|k_1,k_2,d) * ... * P(k_n|k_1,...,k_{n-1},d)
$$

为了提升效率，我们也使用马尔科夫假设和多元文法。假设是三元文法，那么我们可以写成这样：

$$
P(q|d) = P(k_1,k_2,k_3,...,k_n|d) = P(k_1|d) * P(k_2|k_1,d) * P(k_3|k_1,k_2,d) * ... * P(k_n|k_{n-1},k_{n-2},d)
$$

**中文分词**

假设整个文档集合是 D，要分词的句子是 s，分词结果为 w1, … wn，那么我们可以求 P(s) 的概率为：

$$
P(s|D) = P(w_1,w_2,w_3,...,w_n|D)=P(w_1|D)*P(w_2|w_1,D)*P(w_3|w_1,w_2,D)*...*P(w_n|w_{n-1},w_{n-2},D)
$$